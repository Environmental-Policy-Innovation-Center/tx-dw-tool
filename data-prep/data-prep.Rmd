---
title: "TX Drinking Water Application - Data Prep"
author: "EmmaLi Tsai and Gabe Watson"
date: "2024-04-03"
last_updated: "2024-04-03"
output: html_document
---

## TO DO: 
## utilities with no county served 
## str_to_title county served 
## add utility name

## Packages 
```{r}
library(tidyverse)
library(aws.s3)
library(sf)
library(data.table)
```

## Data lists
```{r}
# loading lists of data 
demo <- aws.s3::s3read_using(readRDS, 
                             object = "s3://tech-team-data/state-drinking-water/TX/clean/TX_demographic_list.RData")
keys <- aws.s3::s3read_using(readRDS, 
                             object = "s3://tech-team-data/state-drinking-water/TX/clean/TX_merging_keys_list.RData")

tx_sab_simplified <- aws.s3::s3read_using(st_read, 
                                   object = "state-drinking-water/TX/clean/app/tx_sab_simplified.geojson",
                                   bucket = "tech-team-data")

gs4_deauth()
URL <- "https://docs.google.com/spreadsheets/d/1bzNPxhL-l6DeGElhG1c70Of8DGAQasMDUuX3rPHVe2A/edit#gid=0"
data_dict <- read_sheet(URL, sheet = "var_names")
```

## Creating test data: 
```{r}
# grabbing some sample stats - adding pop categories and ownership type 
# TO DO: clean this section up and make dedicate spaces for variable additions/modifications


owner_type_code <- c("F","L","M","N","P","S")
## from sdwis
owner_type_description <-c("Federal Government","Local Government","Public/Private","Native American","Private","State Government")
owner_type_merge <- data.frame(owner_type_code,owner_type_description)

analysis_keys <- keys$analysis_keys 
sabs <- demo$census %>%
        mutate(pop_catagories = ifelse(estimate_total_pop > 0, "Very Small 0-1k", "Very Small (0 - 1,000)"))%>%
        mutate(pop_catagories = ifelse(estimate_total_pop >= 1001, "Small 1k-3.3k", pop_catagories))%>%
        mutate(pop_catagories = ifelse(estimate_total_pop >= 3301, "Medium 3.3k-10k", pop_catagories))%>%
        mutate(pop_catagories = ifelse(estimate_total_pop >= 10001, "Large 10k-100k", pop_catagories))%>%
        mutate(pop_catagories = ifelse(estimate_total_pop > 100000, "Very Large 100k+", pop_catagories))%>%
        left_join(owner_type_merge)%>%
  select(pwsid, pws_name, county_served, area_miles, pop_density, estimate_total_pop,
         estimate_hisp_alone_per, estimate_laborforce_unemployed_per, 
         estimate_hh_below_pov_per, pop_catagories,owner_type_description,primary_source_code, tier) %>%
  as.data.frame() %>%
  mutate(county_served = str_to_title(county_served))%>%
  mutate(pws_name = str_replace(pws_name,"WSC","Water Supply Corporation"))

# reorganizing for clarity: 
app_test_df <- analysis_keys %>%
  left_join(sabs) %>%
  relocate(primary_source_code:pop_density, .after = east_tx_flag)  %>%
  relocate(estimate_total_pop:estimate_hh_below_pov_per, .after = estimate_mhi)
```


## Adding TX regions: 
```{r}
# script for pulling TX regions: 
file_loc <- "./data-prep/TX_regions"
download.file("https://www.twdb.texas.gov/mapping/gisdata/doc/RWPA_Shapefile.zip", 
              destfile = paste0(file_loc, ".zip"))
unzip(zipfile = paste0(file_loc, ".zip"), exdir = file_loc) 
file.remove(paste0(file_loc, ".zip"))

# grabbing region boundaries 
## EmmaLi - can you put this file in an S3 bucket to preserve code runnability? 
region_boundaries <- st_read(paste0(file_loc, "/TWDB_RWPAs_2014.shp")) %>%
  janitor::clean_names() %>%
  st_transform('+proj=longlat +datum=WGS84')

# saving region boundaries to s3: 
tmp <- tempfile()
st_write(region_boundaries, dsn = paste0(tmp, ".geojson"))
on.exit(unlink(tmp))
put_object(
  file = paste0(tmp, ".geojson"),
  object = "/state-drinking-water/TX/clean/app/TX_regions.geojson",
  bucket = "tech-team-data",
  acl = "public-read"
)

# reading region boundaries from s3: 
region_boundaries <- aws.s3::s3read_using(st_read, 
                                          object = "s3://tech-team-data/state-drinking-water/TX/clean/app/TX_regions.geojson")

# grabbing service areas for st_intersection: 
sabs <- demo$census %>%
  st_transform('+proj=longlat +datum=WGS84')

# running an st_intersection: 
sab_region <- st_intersection(sabs, region_boundaries)

# tidying and reformatting data to a simple region vector: 
sab_region_simple <- sab_region %>%
  as.data.frame() %>%
  select(-geometry) %>%
  group_by(pwsid) %>%
  reframe(regions = paste0(unique(label_2))) %>%
  unnest(regions)

sab_region_tidy <- aggregate(regions ~ pwsid, unique(sab_region_simple), 
                             paste, collapse = ", ")

# saving data frame containing pwsids and region intersections to s3 
tmp <- tempfile()
write.csv(sab_region_tidy, file = paste0(tmp, ".csv"), row.names = FALSE)
on.exit(unlink(tmp))
put_object(
  file = paste0(tmp, ".csv"),
  object = "/state-drinking-water/TX/clean/app/TX_pwsid_regions.csv",
  bucket = "tech-team-data",
  acl = "public-read"
)

# reading pwsid regions from s3: 
pwsid_regions <- aws.s3::s3read_using(read.csv, 
                                      object = "s3://tech-team-data/state-drinking-water/TX/clean/app/TX_pwsid_regions.csv")

# there's one less pwsid because it's over the border in AK (Texarkana region)
```

## Adding extra datasets to the application: 
```{r}
# want to add: bwn, water shortages, dwsrf analysis, water rates, and CVI
# pulling associated data lists: 
wds <- aws.s3::s3read_using(readRDS, 
                            object = "s3://tech-team-data/state-drinking-water/TX/clean/TX_water_delivery_list.RData")
fin <- aws.s3::s3read_using(readRDS, 
                            object = "s3://tech-team-data/state-drinking-water/TX/clean/TX_financial_list.RData")

bwn <- wds$bwn_f2
bwn_f1 <- wds$bwn_f1
shortages <- wds$water_restrictions 
dw_srf <- fin$TX_DWSRF_Analysis %>%
  filter(state == "Texas") %>%
  filter(pwsid != "")
rates <- fin$TX_affordability_DAC
cvi <- demo$cvi

# skeleton dataframe of pwsids: 
pwsids <- demo$census %>% as.data.frame() %>% select(pwsid)

# Rates: ######################################################################
rates$total_water_sewer <- rates$total_water_year + rates$total_sewer_year
pwsid_rates <- rates %>% 
  as.data.frame() %>%
  select(pwsid, total_water_sewer)

extra_vars <- merge(pwsids, pwsid_rates, by = "pwsid", all.x = T)

# CVI: ########################################################################
cvi_tidy <- demo$cvi %>%
  select(-X) %>%
  rename(census_tract = fips_code)

# translate 2010 tract IDs to 2020 tract IDs: 
tract_2010_2020_crosswalk <- read.table("https://www2.census.gov/geo/docs/maps-data/data/rel2020/tract/tab20_tract20_tract10_natl.txt", 
                                        sep = "|", header = TRUE)

cvi_crosswalk <- merge(cvi_tidy, tract_2010_2020_crosswalk, 
                       by.x = "census_tract", 
                       by.y = "GEOID_TRACT_10", all.x = TRUE)

# grab tract geographies: 
tract_geo <- tidycensus::get_acs(
  geography = "tract", 
  variables = c(total_pop = "B01003_001"), 
  state = c("TX"), 
  year = 2020,
  geometry = TRUE
)

cvi_tidy_final <- merge(tract_geo, cvi_crosswalk, by.x = "GEOID", by.y = "GEOID_TRACT_20", all.y = T)

# using areal weighted interpolation to calculate the weighted mean CVI
cvi_geo <- cvi_tidy_final %>%
  st_as_sf() %>%
  st_transform(., crs = "ESRI:102296") 

census_sf <- demo$census %>%
  st_transform(., crs = "ESRI:102296") %>%  
  filter(!st_is_empty(.))%>%
  select(pwsid, geometry)

interpolate <- areal::aw_interpolate(census_sf, tid="pwsid", 
                                     source=cvi_geo, 
                                     sid="census_tract", 
                                     weight = "sum",
                                     output="sf",
                                     intensive=c("overall_cvi_score"))

cvi_weighted <- interpolate %>%
  rename(cvi_weighted_score = overall_cvi_score) %>% 
  as.data.frame() %>%
  select(pwsid, cvi_weighted_score)

# updating extra var dataset: 
extra_vars <- merge(extra_vars, cvi_weighted, all.x = T)

# Limited Water Use ############################################################
limited_water_use <- shortages %>%
  group_by(pwsid) %>%
  summarize(limited_water_use = length(unique(date_notified)))

extra_vars <- merge(extra_vars, limited_water_use, all.x = T) %>%
  # these are true zeroes: 
  mutate(limited_water_use = case_when(is.na(limited_water_use) ~ 0, 
                                       TRUE ~ limited_water_use))

# DW SRFs #####################################################################
dw_srf_simple <- dw_srf %>% 
  group_by(pwsid) %>%
  summarize(dwsrf_times_funded = n(),
            dwsrf_total_assistance = sum(assistance_amt),
            dwsrf_total_pf = sum(prin_forgive_amt),
            dwsrf_median_assistance = median(assistance_amt)) 

extra_vars_final <- merge(extra_vars, dw_srf_simple, all.x = T) %>%
  mutate(dwsrf_times_funded = case_when(is.na(dwsrf_times_funded) ~ 0, 
    TRUE ~ dwsrf_times_funded))


# Boil Water Notices: ##########################################################
# WORK IN PROGRESS: 
head(bwn)
bwn_tidy <- bwn %>%
  rename(pwsid = pws_id) 

bwn_tidy$updtts <- as.Date(bwn_tidy$updtts, tryFormats = c("%Y-%m-%d"))


```

## Joining to simplified GeoJson for quicker load times on app
## Joining Regions 
## Simplified using Mapshapper.com - can likely automate this - went from 71.1mb to 11.5mb!
```{r}
app_test_df_simplified <- app_test_df %>%
                    data.frame()%>%
                    select(-c(geometry))%>%
                    left_join(.,tx_sab_simplified)%>%
                    left_join(.,pwsid_regions) %>%
  left_join(., extra_vars_final, by = "pwsid")
```
## Saving test data to s3: 
```{r}
tmp <- tempfile()
st_write(app_test_df_simplified, dsn = paste0(tmp, ".geojson"))
on.exit(unlink(tmp))
put_object(
  file = paste0(tmp, ".geojson"),
  object = "/state-drinking-water/TX/clean/app/app_test_data_simplified.geojson",
  bucket = "tech-team-data",
  acl = "public-read"
)
```

```{r}
filename <- "data_dict.csv"
fwrite(data_dict, file.path(tempdir(), filename))

put_object(
  file = file.path(tempdir(), filename),
  object = "/state-drinking-water/TX/clean/app/data_dict.csv",
  bucket = "tech-team-data",
  acl = "public-read"
)
```

