---
title: "TX Drinking Water Application - Data Prep"
author: "EmmaLi Tsai and Gabe Watson"
date: "2024-04-03"
last_updated: "2024-04-03"
output: html_document
---

## TO DO: 
## utilities with no county served 
## str_to_title county served 
## add utility name

## Packages 
```{r}
library(tidyverse)
library(aws.s3)
library(sf)
library(data.table)
library(googlesheets4)
```

## Data lists
```{r}
# loading lists of data 
demo <- aws.s3::s3read_using(readRDS, 
                             object = "s3://tech-team-data/state-drinking-water/TX/clean/TX_EPAdemographic_list.RData")
keys <- aws.s3::s3read_using(readRDS, 
                             object = "s3://tech-team-data/state-drinking-water/TX/clean/TX_EPAmerging_keys_list.RData")

# loading simple sabs datset:
tx_sab_simplified <- aws.s3::s3read_using(st_read, 
                                   object = "state-drinking-water/TX/clean/app/tx-sab-simple.geojson",
                                   bucket = "tech-team-data")

# loading latest data dictionary 
gs4_deauth()
URL <- "https://docs.google.com/spreadsheets/d/1jhf-aNJB_91td4YvxaQS04olGYr4IDWyC-dC7UJP5FA/edit?gid=0#gid=0"
data_dict <- read_sheet(URL, sheet = "var_names") %>% 
  mutate(vintage = as.character(vintage))
```

## Creating base application data: 
```{r}
# Adding population categories and translating owner and source codes to 
# human-readable names: 

# owner codes: 
owner_type_code <- c("F","L","M","N","P","S")
## from sdwis
owner_type_description <-c("Federal Government","Local Government","Public/Private","Native American","Private","State Government")
owner_type_merge <- data.frame(owner_type_code,owner_type_description)

# tidying sabs dataset and grabbing key variables for the application: 
analysis_keys <- keys$analysis_keys 

# refactoring columns: 
sabs <- demo$census %>%
  mutate(pop_categories = ifelse(estimate_total_pop > 0, "Very Small 0-1k", "Very Small (0 - 1,000)"))%>%
  mutate(pop_categories = ifelse(estimate_total_pop >= 1001, "Small 1k-3.3k", pop_categories))%>%
  mutate(pop_categories = ifelse(estimate_total_pop >= 3301, "Medium 3.3k-10k", pop_categories))%>%
  mutate(pop_categories = ifelse(estimate_total_pop >= 10001, "Large 10k-100k", pop_categories))%>%
  mutate(pop_categories = ifelse(estimate_total_pop > 100000, "Very Large 100k+", pop_categories))%>%
  # creating more human-readable source type codes: 
  mutate(primary_source_code = case_when(primary_source_code == "SWP" ~ "Surface-Purchased", 
                                         primary_source_code == "GW" ~ "Ground", 
                                         primary_source_code == "SW" ~ "Surface", 
                                         primary_source_code == "GWP" ~ "Ground-Purchased", 
                                         primary_source_code == "GU" ~ "Ground Influenced by Surface")) %>%
  left_join(owner_type_merge)%>%
  select(pwsid, pws_name, county_served, area_miles, pop_density, estimate_total_pop,
         # grabbing all races
         estimate_white_alone_per:estimate_mixed_alone_per, 
         # grabbing age < 5
         estimate_ageunder_5_per,
         # grabbing education categories 
         estimate_bachelors_per:estimate_prof_degree_per,
         estimate_hisp_alone_per, estimate_laborforce_unemployed_per, 
         estimate_hh_below_pov_per, pop_categories, owner_type_description, 
         # grabbing method as a swap for tier 
         primary_source_code, method) %>%
  as.data.frame() %>%
  mutate(county_served = str_to_title(county_served))%>%
  mutate(pws_name = str_replace(pws_name,"WSC","Water Supply Corporation"))

# reorganizing for clarity: 
app_test_df <- analysis_keys %>%
  left_join(sabs) %>%
  relocate(pws_name:county_served, .after = pwsid) %>%
  relocate(pop_categories:method, .after = county_served)  %>%
  relocate(estimate_total_pop:estimate_hh_below_pov_per, .after = estimate_mhi)
```


## Adding TX regions: 
```{r}
# script for pulling TX regions - edited to run regardless of where you've set 
# your working directory (previously it was set to app parent folder)
file_loc <- "TX_regions"
download.file("https://www.twdb.texas.gov/mapping/gisdata/doc/RWPA_Shapefile.zip",
              destfile = paste0(file_loc, ".zip"))
unzip(zipfile = paste0(file_loc, ".zip"), exdir = file_loc)
file.remove(paste0(file_loc, ".zip"))

# grabbing region boundaries 
region_boundaries <- st_read(paste0(file_loc, "/TWDB_RWPAs_2014.shp")) %>%
  janitor::clean_names() %>%
  st_transform('+proj=longlat +datum=WGS84')

# saving region boundaries to s3: 
tmp <- tempfile()
st_write(region_boundaries, dsn = paste0(tmp, ".geojson"))
on.exit(unlink(tmp))
put_object(
  file = paste0(tmp, ".geojson"),
  object = "/state-drinking-water/TX/clean/app/tx-region-boundaries.geojson",
  bucket = "tech-team-data",
  acl = "public-read"
)

# reading region boundaries from s3: 
region_boundaries <- aws.s3::s3read_using(st_read, 
                                          object = "s3://tech-team-data/state-drinking-water/TX/clean/app/tx-region-boundaries.geojson")

# grabbing service areas for st_intersection: 
sabs <- demo$census %>%
  st_transform('+proj=longlat +datum=WGS84')

# running an st_intersection: 
sab_region <- st_intersection(sabs, region_boundaries)

# tidying and reformatting data to a simple region vector: 
sab_region_simple <- sab_region %>%
  as.data.frame() %>%
  select(-geometry) %>%
  group_by(pwsid) %>%
  reframe(regions = paste0(unique(label_2))) %>%
  unnest(regions)

sab_region_tidy <- aggregate(regions ~ pwsid, unique(sab_region_simple), 
                             paste, collapse = ", ")

# saving data frame containing pwsids and region intersections to s3 
tmp <- tempfile()
write.csv(sab_region_tidy, file = paste0(tmp, ".csv"), row.names = FALSE)
on.exit(unlink(tmp))
put_object(
  file = paste0(tmp, ".csv"),
  object = "/state-drinking-water/TX/clean/app/tx-EPApwsid-regions.csv",
  bucket = "tech-team-data",
  acl = "public-read"
)

# reading pwsid regions from s3: 
pwsid_regions <- aws.s3::s3read_using(read.csv, 
                                      object = "s3://tech-team-data/state-drinking-water/TX/clean/app/tx-EPApwsid-regions.csv")

# there's one less pwsid because it's over the border in AK (Texarkana region)
```

## Adding extra variables to the application data: 
```{r}
# Loading extra datasets: ######################################################
# pulling associated data lists for financial and water deliver system info: 
wds <- aws.s3::s3read_using(readRDS, 
                            object = "s3://tech-team-data/state-drinking-water/TX/clean/TX_water_delivery_list.RData")
fin <- aws.s3::s3read_using(readRDS, 
                            object = "s3://tech-team-data/state-drinking-water/TX/clean/TX_financial_list.RData")

# skeleton dataframe of pwsids to store new variables: 
pwsids <- demo$census %>% 
  as.data.frame() %>% 
  select(pwsid)


# Rates: ######################################################################
rates <- fin$TX_affordability_DAC

# grabbing total annual cost: 
rates$total_water_sewer <- rates$total_water_year + rates$total_sewer_year
pwsid_rates <- rates %>% 
  as.data.frame() %>%
  select(pwsid, total_water_sewer)

extra_vars <- merge(pwsids, pwsid_rates, by = "pwsid", all.x = T)


# CVI: ########################################################################
cvi_tidy <- demo$cvi %>%
  select(-X) %>%
  rename(census_tract = fips_code)

# translate 2010 tract IDs to 2020 tract IDs: 
tract_2010_2020_crosswalk <- read.table("https://www2.census.gov/geo/docs/maps-data/data/rel2020/tract/tab20_tract20_tract10_natl.txt", 
                                        sep = "|", header = TRUE)

cvi_crosswalk <- merge(cvi_tidy, tract_2010_2020_crosswalk, 
                       by.x = "census_tract", 
                       by.y = "GEOID_TRACT_10", all.x = TRUE)

# grab tract geographies: 
tract_geo <- tidycensus::get_acs(
  geography = "tract", 
  variables = c(total_pop = "B01003_001"), 
  state = c("TX"), 
  year = 2020,
  geometry = TRUE
)

cvi_tidy_final <- merge(tract_geo, cvi_crosswalk, by.x = "GEOID", 
                        by.y = "GEOID_TRACT_20", all.y = T)

# using areal weighted interpolation to calculate the weighted mean CVI
cvi_geo <- cvi_tidy_final %>%
  st_as_sf() %>%
  st_transform(., crs = "EPSG:3663") 

census_sf <- demo$census %>%
  st_transform(., crs = "EPSG:3663") %>%
  filter(!st_is_empty(.))%>%
  select(pwsid, geometry)

interpolate <- areal::aw_interpolate(census_sf, tid="pwsid", 
                                     source=cvi_geo, 
                                     sid="census_tract", 
                                     weight = "sum",
                                     output="sf",
                                     intensive=c("overall_cvi_score"))

cvi_weighted <- interpolate %>%
  rename(cvi_weighted_score = overall_cvi_score) %>% 
  as.data.frame() %>%
  select(pwsid, cvi_weighted_score)

# updating extra var dataset: 
extra_vars <- merge(extra_vars, cvi_weighted, all.x = T)


# Limited Water Use ############################################################
shortages <- wds$water_restrictions 

# adding data from previous webscrape (which includes 2023 data)
old_shortages <- aws.s3::s3read_using(read.csv, 
                                      object = "s3://tech-team-data/state-drinking-water/TX/raw/TX_water_restrictions.csv") %>%
  select(-X) %>%
  mutate(date_notified = as.Date(date_notified, tryFormats = c("%Y-%m-%d")))

# can confirm that they don't overlap, so we can safely rbind: 
all_shortages <- rbind(old_shortages, shortages)

# grabbing summary stats for pwsids: 
limited_water_use <- all_shortages %>%
  group_by(pwsid) %>%
  summarize(limited_water_use = length(unique(date_notified)))

# merging with dataset: 
extra_vars <- merge(extra_vars, limited_water_use, all.x = T) %>%
  # these are true zeroes: 
  mutate(limited_water_use = case_when(is.na(limited_water_use) ~ 0, 
                                       TRUE ~ limited_water_use))


# DW SRFs #####################################################################
dw_srf <- fin$TX_DWSRF_Analysis %>%
  filter(state == "Texas") %>%
  filter(pwsid != "")

# grabbing assistance summary stats: 
dw_srf_simple <- dw_srf %>% 
  group_by(pwsid) %>%
  summarize(dwsrf_times_funded = n(),
            dwsrf_total_assistance = sum(assistance_amt),
            dwsrf_total_pf = sum(prin_forgive_amt),
            dwsrf_median_assistance = median(assistance_amt)) 

# merging with dataset
extra_vars <- merge(extra_vars, dw_srf_simple, all.x = T) %>%
  mutate(dwsrf_times_funded = case_when(is.na(dwsrf_times_funded) ~ 0, 
    TRUE ~ dwsrf_times_funded))


# Boil Water Notices: ##########################################################
bwn <- wds$bwn_f2

# fix how the dates were read into R
bwn_tidy <- bwn %>%
  rename(pwsid = pws_id) %>%
  mutate(updtts = as.Date(updtts, tryFormats = c("%Y-%m-%d")), 
         reported_date = openxlsx::convertToDate(reported_date),
         achieved_date = openxlsx::convertToDate(achieved_date)) %>%
  # removing this date - presumably a human error since this hasn't occurred yet
  # as of writing
  filter(reported_date != "2024-12-27")

# total of UNIQUE reported bwn dates for each pwsid since 2018
bwn_tidy_pwsid <- bwn_tidy %>%
  group_by(pwsid) %>%
  summarize(total_bwn = length(unique(reported_date)))

extra_vars_v1 <- merge(extra_vars, bwn_tidy_pwsid, all.x = T) %>%
  mutate(total_bwn = case_when(is.na(total_bwn) ~ 0, 
                               TRUE ~ total_bwn))


# EJ screen - proximity to water hazards #######################################
# grabbing data from EJ screen: 
ejscreen <- demo$ejscreen %>%
  select(-X)

# variable keys: 
# Superfund sites (NPL), risk management plan (RMP) proximity, 
# haz waste (TSDF and LQGs) proximity, underground storage tanks (UST), 
# and waste water discharges indicator 
water_haz <- ejscreen %>%
  select(oid, id, pnpl, prmp, ptsdf, ust, pwdis) %>%
  rename(superfund = pnpl, 
         rmp = prmp, 
         haz_waste = ptsdf, 
         storage_tanks = ust, 
         waste_discharge = pwdis)

# need census tract geographies for areal interpolation:  
tract_geo <- tidycensus::get_acs(
  geography = "tract", 
  variables = c(total_pop = "B01003_001"), 
  state = c("TX"), 
  year = 2020,
  geometry = TRUE
)

ejscreen_geo <- merge(water_haz, tract_geo, by.x = "id", by.y = "GEOID")

# using areal interpolation to calculate mean proximity to water hazards: 
ejscreen_geo <- ejscreen_geo %>%
  st_as_sf() %>%
  st_transform(., crs = "EPSG:3663") 

census_sf <- demo$census %>%
  st_transform(., crs = "EPSG:3663") %>%  
  filter(!st_is_empty(.))%>%
  select(pwsid, geometry)

interpolate <- areal::aw_interpolate(census_sf, tid="pwsid", 
                                     source=ejscreen_geo, 
                                     sid="id", 
                                     weight = "sum",
                                     output="sf",
                                     intensive=c("superfund", 
                                                 "rmp", "haz_waste", 
                                                 "storage_tanks",
                                                 "waste_discharge"))

# tidying names: 
ej_inter_tidy <- interpolate %>%
  as.data.frame() %>%
  select(-geometry) 

# adding to extra vars df: 
extra_vars_v1 <- merge(extra_vars_v1, ej_inter_tidy, all.x = T) 


# CEJST Disadvantaged Status ###################################################
cejst <- demo$cejst

# need census tract geographies: 
tract_geo <- tidycensus::get_acs(
  geography = "tract", 
  variables = c(total_pop = "B01003_001"), 
  state = c("TX"), 
  year = 2020,
  geometry = TRUE
)

# CEJST was made off of 2010 tract boundaries - will need to be crosswalked
# for the crosswalk 
tract_2010_2020_crosswalk <- read.table("https://www2.census.gov/geo/docs/maps-data/data/rel2020/tract/tab20_tract20_tract10_natl.txt", 
                                        sep = "|", header = TRUE)

full_crosswalk <- merge(tract_geo, tract_2010_2020_crosswalk, 
                        by.x = "GEOID", 
                        by.y = "GEOID_TRACT_20", all.x = TRUE)

cejst_2020 <- merge(cejst, full_crosswalk, by.x = "census_tract_2010_id", 
                    by.y = "GEOID_TRACT_10", 
                    all.x = T) %>%
  select(GEOID, NAME, 
         total_threshold_criteria_exceeded:percentage_of_tract_that_is_disadvantaged_by_area, 
         geometry)

# using areal weighted interpolation to calculate the % of pwsid that 
# is disadv by area and mean number of threshold criteria that were 
# exceeded: 
cejst_geo <- cejst_2020 %>%
  st_as_sf() %>%
  st_transform(., crs = "EPSG:3663") 

census_sf <- demo$census %>%
  st_transform(., crs = "EPSG:3663") %>%  
  filter(!st_is_empty(.))%>%
  select(pwsid, geometry)

cejst_interpolate <- areal::aw_interpolate(census_sf, tid="pwsid", 
                                           source=cejst_geo, 
                                           sid="GEOID", 
                                           weight = "sum",
                                           output="sf", 
                                           intensive=c("percentage_of_tract_that_is_disadvantaged_by_area", 
                                                       "total_threshold_criteria_exceeded"))
# renaming for clarity: 
cejst_tidy <- cejst_interpolate %>%
  as.data.frame() %>%
  select(-geometry) %>%
  rename(percent_disadv_cejst = percentage_of_tract_that_is_disadvantaged_by_area, 
         mean_thresholds_exceeded_cejst = total_threshold_criteria_exceeded)

# NOTE: some have 100% disadv status but 0 mean thresholds exceeded because: 
# "In addition, a census tract that is completely surrounded by disadvantaged communities that meet the
# burden thresholds—and is at or above the 50th percentile for low income—is also considered
# disadvantaged." - from CEJST documentation 

# adding to extra vars df: 
extra_vars_final <- merge(extra_vars_v1, cejst_tidy, all.x = T) 


# Active health-based violations ###############################################
sdwis <- wds$sdwis

# grab instances where there is a health-based violation and where the RTC 
# column is blank - this interpretation was cleared by EPA staff 
hbv_pwsid_summary <- sdwis %>%
  filter(is_health_based_ind == "Y") %>%
  mutate(open_health_viol = case_when(rtc_date == "" ~ "Yes",
                                      TRUE ~ "No")) %>%
  select(pwsid, open_health_viol) %>%
  filter(open_health_viol == "Yes") %>%
  unique() %>%
  filter(pwsid %in% sabs$pwsid)

# merging with dataset 
extra_vars_final <- merge(extra_vars_final, hbv_pwsid_summary, all.x = T) %>%
  mutate(open_health_viol = case_when(is.na(open_health_viol) ~ "No", 
                               TRUE ~ open_health_viol))

# Year funded ##################################################################

srf_year_funded <- dw_srf %>%
  group_by(pwsid) %>%
  summarize(last_year_funded = max(year))

# final dataset before joining with the base variables: 
extra_vars_final <- merge(extra_vars_final, srf_year_funded, all.x = T) %>%
  relocate(last_year_funded, .after = dwsrf_median_assistance)
```

## Joining to simplified GeoJson for quicker load times on app
## Joining Regions 
## Simplified using Mapshapper.com - can likely automate this - went from 71.1mb to 11.5mb!
```{r}
app_test_df_simplified <- app_test_df %>%
  data.frame()%>%
  select(-c(geometry))%>%
  left_join(., tx_sab_simplified)%>%
  left_join(., pwsid_regions) %>%
  left_join(., extra_vars_final, by = "pwsid")
```

## Saving test data to s3: 
```{r}
tmp <- tempfile()
st_write(app_test_df_simplified, dsn = paste0(tmp, ".geojson"))
on.exit(unlink(tmp))
put_object(
  file = paste0(tmp, ".geojson"),
  object = "/state-drinking-water/TX/clean/app/app-data-dev.geojson",
  bucket = "tech-team-data",
  acl = "public-read"
)
```

## Saving data dictionary to s3: 
```{r}
filename <- "app-data-dict-dev.csv"
data.table::fwrite(data_dict, file.path(tempdir(), filename))

put_object(
  file = file.path(tempdir(), filename),
  object = "/state-drinking-water/TX/clean/app/app-data-dict-dev.csv",
  bucket = "tech-team-data",
  acl = "public-read"
)
```

## Saving report to s3: 
```{r}
# NOTE: just a quick chunk to update the RMD on S3 with local changes - not 
# the best solution yet 
filepath <- "./app/tx-report.Rmd"

put_object(
  file = file.path(filepath),
  object = "/state-drinking-water/TX/clean/app/tx-report-dev.Rmd",
  bucket = "tech-team-data",
  acl = "public-read"
)
```

## Saving methods to s3: 
```{r}
drive_deauth()
# Methods
methods_doc <- drive_download("https://docs.google.com/document/d/1va2Iq2oJxnqiwgNHD4bWpXKxdWbq-TYoYkosj1oz_JU/edit",
                              file.path(tempdir(),"tx-app-methods.docx"), overwrite = TRUE)

put_object(
  file =  file.path(tempdir(),"tx-app-methods.docx"),
  object = "/state-drinking-water/TX/clean/app/tx-app-methods.docx",
  bucket = "tech-team-data",
  acl = "public-read"
)
```
