---
title: "TX Drinking Water Application - Data Prep"
author: "EmmaLi Tsai and Gabe Watson"
date: "2024-04-03"
last_updated: "2024-04-03"
output: html_document
---

## TO DO: 
## utilities with no county served 
## str_to_title county served 
## add utility name

## Packages 
```{r}
library(tidyverse)
library(aws.s3)
library(sf)
```

## Data lists
```{r}
# loading lists of data 
demo <- aws.s3::s3read_using(readRDS, 
                             object = "s3://tech-team-data/state-drinking-water/TX/clean/TX_demographic_list.RData")
keys <- aws.s3::s3read_using(readRDS, 
                             object = "s3://tech-team-data/state-drinking-water/TX/clean/TX_merging_keys_list.RData")

tx_sab_simplified <- aws.s3::s3read_using(st_read, 
                                   object = "state-drinking-water/TX/clean/app/tx_sab_simplified.geojson",
                                   bucket = "tech-team-data")
```

## Creating test data: 
```{r}
# grabbing some sample stats - adding pop categories and ownership type 
# TO DO: clean this section up and make dedicate spaces for variable additions/modifications


owner_type_code <- c("F","L","M","N","P","S")
## from sdwis
owner_type_description <-c("Federal Government","Local Government","Public/Private","Native American","Private","State Government")
owner_type_merge <- data.frame(owner_type_code,owner_type_description)

analysis_keys <- keys$analysis_keys 
sabs <- demo$census %>%
        mutate(pop_catagories = ifelse(estimate_total_pop > 0, "Very Small 0-1k", "Very Small (0 - 1,000)"))%>%
        mutate(pop_catagories = ifelse(estimate_total_pop >= 1001, "Small 1k-3.3k", pop_catagories))%>%
        mutate(pop_catagories = ifelse(estimate_total_pop >= 3301, "Medium 3.3k-10k", pop_catagories))%>%
        mutate(pop_catagories = ifelse(estimate_total_pop >= 10001, "Large 10k-100k", pop_catagories))%>%
        mutate(pop_catagories = ifelse(estimate_total_pop > 100000, "Very Large 100k+", pop_catagories))%>%
        left_join(owner_type_merge)%>%
  select(pwsid, county_served, area_miles, pop_density, estimate_total_pop,
         estimate_hisp_alone_per, estimate_laborforce_unemployed_per, 
         estimate_hh_below_pov_per, pop_catagories,owner_type_description,primary_source_code, tier) %>%
  as.data.frame() 

# reorganizing for clarity: 
app_test_df <- analysis_keys %>%
  left_join(sabs) %>%
  relocate(primary_source_code:pop_density, .after = east_tx_flag)  %>%
  relocate(estimate_total_pop:estimate_hh_below_pov_per, .after = estimate_mhi)
```


## Joining to simplified GeoJson for quicker load times on app
## Simplified using Mapshapper.com - can likely automate this - went from 71.1mb to 11.5mb!
```{r}
app_test_df_simplified <- app_test_df %>%
                    data.frame()%>%
                    select(-c(geometry))%>%
                    left_join(.,tx_sab_simplified)

```
## Saving test data to s3: 
```{r}
tmp <- tempfile()
st_write(app_test_df_simplified, dsn = paste0(tmp, ".geojson"))
on.exit(unlink(tmp))
put_object(
  file = paste0(tmp, ".geojson"),
  object = "/state-drinking-water/TX/clean/app/app_test_data_simplified.geojson",
  bucket = "tech-team-data",
  acl = "public-read"
)
```

## Adding TX regions: 
```{r}
# script for pulling TX regions: 
file_loc <- "./data-prep/TX_regions"
download.file("https://www.twdb.texas.gov/mapping/gisdata/doc/RWPA_Shapefile.zip", 
              destfile = paste0(file_loc, ".zip"))
unzip(zipfile = paste0(file_loc, ".zip"), exdir = file_loc) 
file.remove(paste0(file_loc, ".zip"))

# grabbing region boundaries 
region_boundaries <- st_read(paste0(file_loc, "/TWDB_RWPAs_2014.shp")) %>%
  janitor::clean_names() %>%
  st_transform('+proj=longlat +datum=WGS84')

# saving region boundaries to s3: 
tmp <- tempfile()
st_write(region_boundaries, dsn = paste0(tmp, ".geojson"))
on.exit(unlink(tmp))
put_object(
  file = paste0(tmp, ".geojson"),
  object = "/state-drinking-water/TX/clean/app/TX_regions.geojson",
  bucket = "tech-team-data",
  acl = "public-read"
)

# reading region boundaries from s3: 
region_boundaries <- aws.s3::s3read_using(st_read, 
                                          object = "s3://tech-team-data/state-drinking-water/TX/clean/app/TX_regions.geojson")

# grabbing service areas for st_intersection: 
sabs <- demo$census %>%
  st_transform('+proj=longlat +datum=WGS84')

# running an st_intersection: 
sab_region <- st_intersection(sabs, region_boundaries)

# tidying and reformatting data to a simple region vector: 
sab_region_simple <- sab_region %>%
  as.data.frame() %>%
  select(-geometry) %>%
  group_by(pwsid) %>%
  reframe(regions = paste0(unique(label_2))) %>%
  unnest(regions)

sab_region_tidy <- aggregate(regions ~ pwsid, unique(sab_region_simple), 
                             paste, collapse = ", ")

# saving data frame containing pwsids and region intersections to s3 
tmp <- tempfile()
write.csv(sab_region_tidy, file = paste0(tmp, ".csv"), row.names = FALSE)
on.exit(unlink(tmp))
put_object(
  file = paste0(tmp, ".csv"),
  object = "/state-drinking-water/TX/clean/app/TX_pwsid_regions.csv",
  bucket = "tech-team-data",
  acl = "public-read"
)

# reading pwsid regions from s3: 
pwsid_regions <- aws.s3::s3read_using(read.csv, 
                                      object = "s3://tech-team-data/state-drinking-water/TX/clean/app/TX_pwsid_regions.csv")
# there's one less pwsid because it's over the border in AK (Texarkana region)
```